#!/bin/bash
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

##### Remember, this is a velocity template 
set -x
umask 0022
echo $$
ps -e -o pid,pgrp,user,args
ps x -o  "%p %r %y %x %c "
chmod -R u+w $logDir
rm -rf $logDir
# makes $logDir and $logDir/tmp
mkdir -p $logDir/tmp
if [[ -n "${javaHomeForTests}" ]]
then
  export JAVA_HOME=$javaHomeForTests
  export PATH=$JAVA_HOME/bin/:$PATH
elif [[ -n "${javaHome}" ]]
then
  export JAVA_HOME=$javaHome
  export PATH=$JAVA_HOME/bin/:$PATH
fi
export ANT_OPTS="-Xmx1g -XX:MaxPermSize=256m -Djava.io.tmpdir=$logDir/tmp ${antEnvOpts}"
export M2_OPTS="-Xmx1g -XX:MaxPermSize=256m -Djava.io.tmpdir=$logDir/tmp ${mavenEnvOpts}"
export HADOOP_ROOT_LOGGER=INFO,console
export HADOOP_OPTS="-Dhive.log.dir=$logDir -Dhive.query.id=hadoop -Djava.io.tmpdir=$logDir/tmp"
cd $localDir/$instanceName/${repositoryName}-source || exit 1
if [[ -s batch.pid ]]
then
  while read pid
  do
    if kill -9 -$pid 2>/dev/null
    then
      echo "Killed process group $pid"
      sleep 1
    fi
  done < batch.pid
fi
echo "$$" > batch.pid
find ./ -name 'TEST-*.xml' -delete
find ./ -name 'hive.log' -delete
find ./ -name junit_metastore_db | xargs -r rm -rf
ret=0
################# SHARK TESTS ############
export SHARK_HOME=$localDir/$instanceName/${repositoryName}-source
WSP_DIR=run-tests-from-scratch-workspace
export TEST=$(echo "testCliDriver_"$(echo ${testArguments}|sed 's/-Dtestcase=TestSharkCliDriver //'|sed 's/-Dqfile=//'|sed 's/.q//g'|sed 's/,/$|testCliDriver_/g')"$")
export SCALA_HOME=${SHARK_HOME}

#SCALA
. /etc/profile.d/scala.sh || true
export SCALA_VERSION=`awk '/val SCALA_VERSION/ {print $4}' < ${SHARK_HOME}/project/SharkBuild.scala |sed -e "s/\"\(.*\)\"/\1/"`
if [ "x$SCALA_HOME" != "x" ] ; then
  # User already specified SCALA_HOME. Make sure the correct version of Scala installed.
     SCALA_HOME_VERSION=`${SCALA_HOME}/bin/scala -version 2>&1 | awk '{print $5}'`
       if [ "$SCALA_HOME_VERSION" != "$SCALA_VERSION" ] ; then
           echo "Your SCALA_HOME is version ${SCALA_HOME_VERSION}, but ${SCALA_VERSION} is required."
               exit -1
       fi
fi
CLASSPATH+=":$SCALA_HOME/lib/scala-library.jar"
CLASSPATH+=":$SCALA_HOME/lib/scala-compiler.jar"
CLASSPATH+=":$SCALA_HOME/lib/jline.jar"
export CLASSPATH

#HIVE
export HIVE_HOME=${SHARK_HOME}/${WSP_DIR}/hive/build/dist
export HIVE_DEV_HOME=${SHARK_HOME}/${WSP_DIR}/hive

#SPARK
export SPARK_HOME=${SHARK_HOME}/${WSP_DIR}/spark
SPARK_CLASSPATH+=":${HIVE_DEV_HOME}/build/ql/test/classes"
SPARK_CLASSPATH+=":${HIVE_DEV_HOME}/build/ivy/lib/test/hadoop-test-0.20.2.jar"
SPARK_CLASSPATH+=":${HIVE_DEV_HOME}/data/conf"
export SPARK_CLASSPATH

SPARK_JAVA_OPTS+='-Dspark.local.dir=/tmp '
SPARK_JAVA_OPTS+='-Dspark.kryoserializer.buffer.mb=10 '
SPARK_JAVA_OPTS+="-verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps "
export SPARK_JAVA_OPTS
export TEST_WITH_ANT=1
export RUNNER="ant -noclasspath -nouserlib -f $SHARK_HOME/bin/dev/build_test.xml test"

HADOOP_VERSION=`echo ${SHARK_HOME}/${WSP_DIR}/hadoop-*/ | sed -e "s/.*hadoop-\(.*\)\//\1/"`
hadoop_version_re="^([[:digit:]]+)\.([[:digit:]]+)(\.([[:digit:]]+))?.*$"

if [[ "$HADOOP_VERSION" =~ $hadoop_version_re ]]; then
    hadoop_major_ver=${BASH_REMATCH[1]}
    hadoop_minor_ver=${BASH_REMATCH[2]}
    hadoop_patch_ver=${BASH_REMATCH[4]}
else
    echo "Unable to determine Hadoop version information. SPARK_HADOOP_VERSION is required."
    exit 1
fi

#TESTS
export BUILD_PATH=$HIVE_DEV_HOME/build/ql
# Set variables used by unit tests (ex. create_like.q).
TEST_JAVA_OPTS="-Dbuild.dir=${HIVE_DEV_HOME}/build/ql "
TEST_JAVA_OPTS+="-Dbuild.dir.hive=${HIVE_DEV_HOME}/build "
TEST_JAVA_OPTS+="-Dbuild.ivy.lib.dir=${HIVE_DEV_HOME}/build/ivy/lib "
TEST_JAVA_OPTS+="-Dderby.version=10.4.2.0 "
TEST_JAVA_OPTS+="-Dlog4j.configuration=file://${HIVE_DEV_HOME}/data/conf/hive-log4j.properties "
TEST_JAVA_OPTS+="-Dtest.log.dir=${BUILD_PATH}/test/logs "
TEST_JAVA_OPTS+="-Dtest.output.overwrite=false "
TEST_JAVA_OPTS+="-Dtest.src.data.dir=${HIVE_DEV_HOME}/data "
TEST_JAVA_OPTS+="-Dtest.tmp.dir=${BUILD_PATH}/tmp "
TEST_JAVA_OPTS+="-Dtest.warehouse.dir=${BUILD_PATH}/test/data/warehouse "

if [[ $hadoop_major_ver -eq 2 && $hadoop_minor_ver -ge 3 ]]; then
  TEST_JAVA_OPTS+="-Dtest.dfs.mkdir=\"-mkdir -p\" "
else
  TEST_JAVA_OPTS+="-Dtest.dfs.mkdir=-mkdir "
fi

export TEST_JAVA_OPTS

if  [[ ! -f $SHARK_HOME/recompile.done ]]
then
  sleep $(expr $instance \* 60)  
  #First time re-compile hive to fix paths to qfiles
  cd ${HIVE_DEV_HOME} 
  ant jar-test 1>$logDir/ant-test.txt 2>&1 </dev/null || true
  touch $SHARK_HOME/recompile.done
fi

cd ${SHARK_HOME}/${WSP_DIR}/hive/ql && sh -x ../../../run 1>$logDir/run-test.txt 2>&1 </dev/null &



#[[
  pid=$!
]]#

cd ${SHARK_HOME}

#####################################
echo $pid >> batch.pid
wait $pid
ret=$?
if [[ $ret -ne 0 ]]
then
  if [[ $numOfFailedTests -lt $maxSourceDirs ]]
  then
    cp -R $localDir/$instanceName/${repositoryName}-source $logDir/source
  else
    echo "Number of failed tests $numOfFailedTests exceeded threshold, not copying source"
  fi
fi

find ./ -type f -name hive.log | \
  xargs -I {} sh -c 'f=$(basename {}); test -f ${logDir}/$f && f=$f-$(uuidgen); mv {} ${logDir}/$f'
find ./ -type f -name 'TEST-*.xml' | \
  xargs -I {} sh -c 'f=TEST-${batchName}-$(basename {}); test -f ${logDir}/$f && f=$f-$(uuidgen); mv {} ${logDir}/$f'
exit $ret
